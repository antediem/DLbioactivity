{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "graph_conv_mfconv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJqGyfKkoHQD"
      },
      "source": [
        "# Instalations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8ML_-68rCrg",
        "outputId": "1ce4f37d-ddcb-4acb-a32a-468e17b60857"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install networkx\n",
        "!pip install pysmiles\n",
        "!curl -L bit.ly/rdkit-colab | tar xz -C /\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (2.5.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx) (4.4.2)\n",
            "Requirement already satisfied: pysmiles in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: networkx~=2.0 in /usr/local/lib/python3.7/dist-packages (from pysmiles) (2.5.1)\n",
            "Requirement already satisfied: pbr in /usr/local/lib/python3.7/dist-packages (from pysmiles) (5.6.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx~=2.0->pysmiles) (4.4.2)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   163  100   163    0     0   3134      0 --:--:-- --:--:-- --:--:--  3134\n",
            "100   133  100   133    0     0    950      0 --:--:-- --:--:-- --:--:--   950\n",
            "100   620  100   620    0     0   2707      0 --:--:-- --:--:-- --:--:--  605k\n",
            "100 29.6M  100 29.6M    0     0  20.1M      0  0:00:01  0:00:01 --:--:-- 26.2M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7y3csK0ptIG"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEl7wzZR0HOb",
        "outputId": "4892d31a-ecd1-4a48-bd92-0697e88d9149"
      },
      "source": [
        "#https://drive.google.com/file/d/18XqI6VT5dLR-m-jJ7fLtv4Y1Uyc9ZtXP/view?usp=sharing\n",
        "#https://drive.google.com/file/d/1Se9qKoSHE24HwYhZfQP5xOWio1fvET6m/view?usp=sharing\n",
        "#https://drive.google.com/file/d/1mN10JygWjyIfEvWP7fro8nHZTLlM2SZJ/view?usp=sharing\n",
        "\n",
        "!gdown --id \"18s872gYgaLXk_sLxySKjsh3FRjNa15L3\" # hiv1_hcv dataset\n",
        "!gdown --id \"1Se9qKoSHE24HwYhZfQP5xOWio1fvET6m\" # flua_hiv1 dataset\n",
        "!gdown --id '1mN10JygWjyIfEvWP7fro8nHZTLlM2SZJ' # "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18s872gYgaLXk_sLxySKjsh3FRjNa15L3\n",
            "To: /content/hiv1_hcv.csv\n",
            "5.45GB [00:51, 106MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Se9qKoSHE24HwYhZfQP5xOWio1fvET6m\n",
            "To: /content/flua_hiv1_fixed.csv\n",
            "3.09GB [00:36, 85.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mN10JygWjyIfEvWP7fro8nHZTLlM2SZJ\n",
            "To: /content/flua_hcv_fixed.csv\n",
            "4.75GB [00:57, 70.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9oWam0_qwll"
      },
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import os, glob\n",
        "\n",
        "from pysmiles import read_smiles\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from torch.utils.data import dataloader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from torch_geometric.utils.convert import from_networkx\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "def mol_to_nx(mol):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        features = [atom.GetAtomicNum(),\n",
        "                   atom.GetMass(),\n",
        "                   atom.GetFormalCharge(),\n",
        "                   hybridization_encoding(atom.GetHybridization()),\n",
        "                   atom.GetNumExplicitHs(),\n",
        "                   atom.GetExplicitValence(), #explicit valence (including Hs)\n",
        "                   atom.GetNumRadicalElectrons(),\n",
        "                   (1 if atom.GetIsAromatic() else 0)]\n",
        "        G.add_node(atom.GetIdx(),\n",
        "                    x=features)\n",
        "    for bond in mol.GetBonds():\n",
        "        G.add_edge(bond.GetBeginAtomIdx(),\n",
        "                   bond.GetEndAtomIdx(),\n",
        "                   bond_type=bond.GetBondType())\n",
        "    return G\n",
        "\n",
        "def hybridization_encoding(hybridization):\n",
        "    if hybridization == Chem.HybridizationType.S:\n",
        "        return 1\n",
        "    if hybridization == Chem.HybridizationType.SP:\n",
        "        return 2\n",
        "    if hybridization == Chem.HybridizationType.SP2:\n",
        "        return 3\n",
        "    if hybridization == Chem.HybridizationType.SP3:\n",
        "        return 4\n",
        "    if hybridization == Chem.HybridizationType.SP3D:\n",
        "        return 5\n",
        "    if hybridization == Chem.HybridizationType.SP3D2:\n",
        "        return 6\n",
        "    if hybridization == Chem.HybridizationType.OTHER:\n",
        "        return 7"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Aoppwr3wcT6"
      },
      "source": [
        "def read_data(data_path):\n",
        "    data = None\n",
        "    if data_path.endswith('.csv'):\n",
        "        try:\n",
        "            data = pd.read_csv(data_path)\n",
        "        except ValueError:\n",
        "            print('ValueError')\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def train_validation_split(data_path):\n",
        "    if os.path.isdir(data_path):\n",
        "        train_path = os.path.join(data_path, 'train.csv')\n",
        "        val_path = os.path.join(data_path, 'val.csv')\n",
        "    else:\n",
        "        train_path = data_path.split('.')[0] + '_' + 'train.csv'\n",
        "        val_path = data_path.split('.')[0] + '_' + 'val.csv'\n",
        "    if os.path.exists(train_path) and os.path.exists(val_path):\n",
        "\n",
        "        return pd.read_csv(train_path), pd.read_csv(val_path)\n",
        "\n",
        "    data = read_data(data_path)\n",
        "    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "    train_data.to_csv(train_path, index=False)\n",
        "    val_data.to_csv(val_path, index=False)\n",
        "\n",
        "    return train_data, val_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmfBCjVHqzp3"
      },
      "source": [
        "class ANYDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, data, infer=False):\n",
        "\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            self.data = data\n",
        "        elif isinstance(data, str):\n",
        "            self.data = read_data(data)\n",
        "\n",
        "        self.NON_MORD_NAMES = ['smiles', 'active']\n",
        "        \n",
        "        scl = StandardScaler()\n",
        "\n",
        "        self.mord_ft = scl.fit_transform(\n",
        "            self.data.drop(columns=self.NON_MORD_NAMES).astype(np.float64)).tolist()\n",
        "\n",
        "        self.graphs = [Chem.MolFromSmiles(s) for s in self.data['smiles'].values.tolist()]\n",
        "        self.graphs = [from_networkx(mol_to_nx(g)) for g in self.graphs]\n",
        "        self.label = self.data['active'].values.tolist()\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        return self.graphs[idx], self.mord_ft[idx], self.label[idx]\n",
        "\n",
        "class Collater(object):\n",
        "\n",
        "    def __init__(self, follow_batch=[], exclude_keys=[]):\n",
        "        self.follow_batch = follow_batch\n",
        "        self.exclude_keys = exclude_keys\n",
        "\n",
        "    def collate(self, batch):\n",
        "        \n",
        "        graph_batch = Batch.from_data_list(list(zip(*batch))[0], self.follow_batch,\n",
        "                                           self.exclude_keys)\n",
        "        md_batch = torch.tensor(list(zip(*batch))[1])\n",
        "        labels_batch = torch.tensor(list(zip(*batch))[2]).long()\n",
        "        \n",
        "        return (graph_batch, md_batch), labels_batch\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self.collate(batch)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyzXAnotxzLP"
      },
      "source": [
        "def get_dataset(dataset_type: str):\n",
        "    if dataset_type == 'flua_hcv':\n",
        "        path2data = '/content/flua_hcv_fixed.csv'\n",
        "        vtoi = {'hcv': 0, 'flua': 1}\n",
        "    if dataset_type == 'hiv1_hcv':\n",
        "        path2data = '/content/hiv1_hcv.csv'\n",
        "        vtoi = {'hcv': 0, 'hiv': 1}\n",
        "    if dataset_type == 'flua_hiv1':\n",
        "        path2data = '/content/flua_hiv1_fixed.csv'\n",
        "        vtoi = {'flua': 0, 'hiv1': 1}\n",
        "\n",
        "    train_data, val_data = train_validation_split(path2data)\n",
        "\n",
        "    train_data = train_data.drop(columns=['smile_ft'])\n",
        "    val_data = val_data.drop(columns=['smile_ft'])\n",
        "    train_data['active'] = train_data['active'].apply(lambda x: vtoi[x])\n",
        "    val_data['active'] = val_data['active'].apply(lambda x: vtoi[x])\n",
        "    to_drop = ['Nc1ccc(cc1)[S+]2(=O)[NH2+]c3nccc[n+]3[AgH3-4]O2']\n",
        "    train_data = train_data[~train_data.smiles.isin(to_drop)]\n",
        "    val_data = val_data[~val_data.smiles.isin(to_drop)]\n",
        "\n",
        "    return train_data, val_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL4X_tIFiBOj"
      },
      "source": [
        "# Define the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX8ZugKpiGwL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "from torch.nn import Linear, ReLU\n",
        "from torch_geometric.nn import Sequential, GCNConv, MFConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GraphNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GraphNet, self).__init__()\n",
        "\n",
        "        self.graph_conv1 = MFConv(8, 32)\n",
        "        self.graph_conv2 = MFConv(32, 32)\n",
        "        self.graph_conv3 = MFConv(32, 32)\n",
        "\n",
        "        self.fc1 = nn.Linear(862, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.linear = nn.Linear(32 + 64, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, x_md):\n",
        "\n",
        "        x = self.graph_conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.graph_conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.graph_conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        \n",
        "        x_md = F.relu(self.fc1(x_md))\n",
        "        x_md = self.batch_norm1(x_md)\n",
        "        x_md = self.dropout(x_md)\n",
        "\n",
        "        x_md = F.relu(self.fc2(x_md))\n",
        "        x_md = self.batch_norm2(x_md)\n",
        "        x_md = self.dropout(x_md)\n",
        "\n",
        "        x_md = F.relu(self.fc3(x_md))\n",
        "        x_md = self.batch_norm3(x_md)\n",
        "        x_md = self.dropout(x_md)\n",
        "\n",
        "        x = torch.cat([x, x_md], dim=1)\n",
        "\n",
        "        return torch.sigmoid(self.linear(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ_7ynBZikQb"
      },
      "source": [
        "# Metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F9fMP8lizib"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "THRESH = 0.2\n",
        "\n",
        "def auc(y_true, y_scores):\n",
        "    y_true = y_true.cpu().detach().numpy()\n",
        "    y_scores = y_scores.cpu().detach().numpy()\n",
        "    return metrics.roc_auc_score(y_true, y_scores)\n",
        "\n",
        "\n",
        "def auc_threshold(y_true, y_scores):\n",
        "    y_true = y_true.cpu().detach().numpy()\n",
        "    y_scores = y_scores.cpu().detach().numpy()\n",
        "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_scores)\n",
        "    return metrics.auc(fpr, tpr)\n",
        "\n",
        "\n",
        "def get_score_obj(y_true, y_scores, thresh=THRESH):\n",
        "    y_true = y_true.cpu().detach().numpy()\n",
        "    y_scores = (y_scores.cpu().detach().numpy() + thresh).astype(np.int16)\n",
        "    return metrics.classification_report(y_true, y_scores, output_dict=True)\n",
        "\n",
        "\n",
        "def f1(y_true, y_scores):\n",
        "    score_obj = get_score_obj(y_true, y_scores)\n",
        "    return score_obj['weighted avg']['f1-score']\n",
        "\n",
        "\n",
        "def sensitivity(y_true, y_scores, thresh=THRESH):\n",
        "    y_true = y_true.cpu().detach().numpy()\n",
        "    y_scores = (y_scores.cpu().detach().numpy() + 1 - thresh).astype(np.int16)\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_scores).ravel()\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "\n",
        "def specificity(y_true, y_scores, thresh=THRESH):\n",
        "    y_true = y_true.cpu().detach().numpy()\n",
        "    y_scores = (y_scores.cpu().detach().numpy() + 1 - thresh).astype(np.int16)\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_scores).ravel()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_scores, thresh=THRESH):\n",
        "    y_true = y_true.cpu().detach().numpy()\n",
        "    y_scores = (y_scores.cpu().detach().numpy() + 1 - thresh).astype(np.int16)\n",
        "    return metrics.accuracy_score(y_true, y_scores)\n",
        "\n",
        "\n",
        "def mcc(y_true, y_scores, thresh=THRESH):\n",
        "    y_true = y_true.cpu().detach().numpy()\n",
        "    y_scores = (y_scores.cpu().detach().numpy() + 1 - thresh).astype(np.int16)\n",
        "    return metrics.matthews_corrcoef(y_true, y_scores)\n",
        "\n",
        "def plot_roc_curve(y_true, y_pred, hashcode=''):\n",
        "\n",
        "    if not os.path.exists('vis/'):\n",
        "        os.makedirs('vis/')\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred, pos_label=1)\n",
        "    auc_roc = metrics.roc_auc_score(y_true, y_pred)\n",
        "    print('AUC: {:4f}'.format(auc_roc))\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.savefig('vis/ROC_{}'.format(hashcode + '.png'))\n",
        "    plt.clf()  # Clear figure\n",
        "\n",
        "\n",
        "def plot_precision_recall(y_true, y_pred, hashcode=''):\n",
        "\n",
        "    if not os.path.exists('vis/'):\n",
        "        os.makedirs('vis/')\n",
        "\n",
        "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
        "    plt.plot(thresholds, precisions[:-1], label=\"Precision\")\n",
        "    plt.plot(thresholds, recalls[:-1], label=\"Recall\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylim([0, 1])\n",
        "    plt.savefig('vis/PR_{}'.format(hashcode + '.png'))\n",
        "    plt.clf()  # Clear figure"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wJNwBeI0mf7"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4UOufJAjInJ"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_max_length(x):\n",
        "    return len(max(x, key=len))\n",
        "\n",
        "\n",
        "def pad_sequence(seq):\n",
        "    def _pad(_it, _max_len):\n",
        "        return [0] * (_max_len - len(_it)) + _it\n",
        "    padded = [_pad(it, get_max_length(seq)) for it in seq]\n",
        "    return padded\n",
        "\n",
        "def create_dir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "\n",
        "\n",
        "def save_pickle(obj, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def read_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "def save_model(model, model_dir_path, hash_code):\n",
        "    if not os.path.exists(model_dir_path):\n",
        "        os.makedirs(model_dir_path)\n",
        "    torch.save(model.state_dict(), \"{}/model_{}_{}\".format(model_dir_path, hash_code, \"BEST\"))\n",
        "    print('Save done!')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woNSUklii0cX"
      },
      "source": [
        "# Train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDb2M8H9llI-"
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorboard_logger\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from torch.utils.data import dataloader\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "models_path = '/content'\n",
        "\n",
        "def train_validate(train_dataset,\n",
        "                          val_dataset,\n",
        "                          train_device,\n",
        "                          val_device,\n",
        "                          opt_type,\n",
        "                          n_epoch,\n",
        "                          batch_size,\n",
        "                          metrics,\n",
        "                          hash_code,\n",
        "                          lr):\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=batch_size, \n",
        "                                               shuffle=True,\n",
        "                                               collate_fn=Collater())\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                             batch_size=batch_size, \n",
        "                                             shuffle=False,\n",
        "                                             collate_fn=Collater())\n",
        "    device = 'cuda:0'\n",
        "    \n",
        "    try:\n",
        "        tensorboard_logger.configure('logs/' + hash_code)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    model = GraphNet().to(device)\n",
        "\n",
        "    if opt_type == 'sgd':\n",
        "        opt = optim.SGD(model.parameters(),\n",
        "                        lr=lr,\n",
        "                        momentum=0.99)\n",
        "        \n",
        "    elif opt_type == 'adam':\n",
        "        opt = optim.Adam(model.parameters(),\n",
        "                         lr=lr)\n",
        "        \n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(opt, [40, 80], gamma=0.1)\n",
        "\n",
        "    min_loss = 100\n",
        "    early_stop_count = 0\n",
        "\n",
        "    for e in range(n_epoch):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_outputs = []\n",
        "        val_outputs = []\n",
        "        train_labels = []\n",
        "        val_labels = []\n",
        "\n",
        "        print(e, '--', 'TRAINING ==============>')\n",
        "\n",
        "        for i, ((mol_graph, md), label) in enumerate(train_loader):\n",
        "            model.train()\n",
        "\n",
        "            mol_graph.x = mol_graph.x.float().to(device)\n",
        "            label = label.float().to(train_device)\n",
        "            mol_graph.edge_index = mol_graph.edge_index.to(train_device)\n",
        "            mol_graph.batch = mol_graph.batch.to(train_device)\n",
        "            md = md.to(device)\n",
        "\n",
        "            # Forward\n",
        "            opt.zero_grad()\n",
        "            outputs = model(mol_graph.x, mol_graph.edge_index, mol_graph.batch, md)\n",
        "            outputs = torch.squeeze(outputs)\n",
        "            \n",
        "            loss = criterion(outputs, label)\n",
        "            train_losses.append(float(loss.item()))\n",
        "            train_outputs.extend(outputs)\n",
        "            train_labels.extend(label)\n",
        "\n",
        "            # Parameters update\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Validate after each epoch\n",
        "        print('EPOCH', e, '--', 'VALIDATION ==============>')\n",
        "        for i, ((mol_graph, md), label) in enumerate(val_loader):\n",
        "            model.eval()\n",
        "\n",
        "            mol_graph.x = mol_graph.x.float().to(device)\n",
        "            label = label.float().to(train_device)\n",
        "            mol_graph.edge_index = mol_graph.edge_index.to(train_device)\n",
        "            mol_graph.batch = mol_graph.batch.to(train_device)\n",
        "            md = md.to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model(mol_graph.x, mol_graph.edge_index, mol_graph.batch, md)\n",
        "                outputs = torch.squeeze(outputs)\n",
        "                \n",
        "                loss = criterion(outputs, label)\n",
        "                val_losses.append(float(loss.item()))\n",
        "                val_outputs.extend(outputs)\n",
        "                val_labels.extend(label)\n",
        "\n",
        "        train_outputs = torch.stack(train_outputs)\n",
        "        val_outputs = torch.stack(val_outputs)\n",
        "        train_labels = torch.stack(train_labels)\n",
        "        val_labels = torch.stack(val_labels)\n",
        "        tensorboard_logger.log_value('train_loss', sum(train_losses) / len(train_losses), e + 1)\n",
        "        tensorboard_logger.log_value('val_loss', sum(val_losses) / len(val_losses), e + 1)\n",
        "\n",
        "        print('{\"metric\": \"train_loss\", \"value\": %f, \"epoch\": %d}' % (sum(train_losses) / len(train_losses), e + 1))\n",
        "        print('{\"metric\": \"val_loss\", \"value\": %f, \"epoch\": %d}' % (sum(val_losses) / len(val_losses), e + 1))\n",
        "        \n",
        "        for key in metrics.keys():\n",
        "            train_metric = metrics[key](train_labels, train_outputs)\n",
        "            val_metric = metrics[key](val_labels, val_outputs)\n",
        "\n",
        "            print('{\"metric\": \"%s\", \"value\": %f, \"epoch\": %d}' % ('train_' + key, train_metric, e + 1))\n",
        "            print('{\"metric\": \"%s\", \"value\": %f, \"epoch\": %d}' % ('val_' + key, val_metric, e + 1))\n",
        "\n",
        "            tensorboard_logger.log_value('train_{}'.format(key),\n",
        "                                         train_metric, e + 1)\n",
        "            tensorboard_logger.log_value('val_{}'.format(key),\n",
        "                                         val_metric, e + 1)\n",
        "            \n",
        "        loss_epoch = sum(val_losses) / len(val_losses)\n",
        "\n",
        "        if loss_epoch < min_loss:\n",
        "            early_stop_count = 0\n",
        "            min_loss = loss_epoch\n",
        "            save_model(model, models_path, hash_code)\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "            if early_stop_count > 30:\n",
        "                print('Traning can not improve from epoch {}\\tBest loss: {}'.format(e, min_loss))\n",
        "                break\n",
        "\n",
        "    train_metrics = {}\n",
        "    val_metrics = {}\n",
        "\n",
        "    for key in metrics.keys():\n",
        "        train_metrics[key] = metrics[key](train_labels, train_outputs)\n",
        "        val_metrics[key] = metrics[key](val_labels, val_outputs)\n",
        "\n",
        "    return train_metrics, val_metrics\n",
        "\n",
        "\n",
        "def predict(dataset, model_path, device='cpu'):\n",
        "    \n",
        "    loader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                             batch_size=128, \n",
        "                                             shuffle=False,\n",
        "                                             collate_fn=Collater())\n",
        "    model = GraphNet().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    # EVAL_MODE\n",
        "\n",
        "    model.eval()\n",
        "    probas = []\n",
        "    for i, ((mol_graph, md), label) in enumerate(loader):\n",
        "        with torch.no_grad():\n",
        "            mol_graph = mol_graph.to(device)\n",
        "            # Forward to get smiles and equivalent weights\n",
        "            proba = model(mol_graph.x.to(device), \n",
        "                          mol_graph.edge_index.to(device), \n",
        "                          mol_graph.batch.to(device), md.to(device)).cpu()\n",
        "            probas.append(proba)\n",
        "    print('Forward done !!!')\n",
        "    probas = np.concatenate(probas)\n",
        "    return probas"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2lRg7JC4QRh"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YW4mDQNO_AAW",
        "outputId": "812434de-86e3-48a7-d61c-f4df47ce309c"
      },
      "source": [
        "#Hashcode for tf.events\n",
        "hashcode = 'TEST'\n",
        "\n",
        "train_data, val_data = get_dataset('flua_hiv1')\n",
        "train_dataset = ANYDataset(train_data)\n",
        "val_dataset = ANYDataset(val_data)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    train_device = 'cuda'\n",
        "    val_device = 'cuda'\n",
        "else:\n",
        "    train_device = 'cpu'\n",
        "    val_device = 'cpu'\n",
        "\n",
        "train_validate(train_dataset,\n",
        "                  val_dataset,\n",
        "                  train_device,\n",
        "                  val_device,\n",
        "                  'adam', #Optimizer adam ('adam') or sgd ('sgd')\n",
        "                  int(500), #Number of epochs\n",
        "                  int(128), #Batch size\n",
        "                  {'sensitivity': sensitivity, 'specificity': specificity,\n",
        "                    'accuracy': accuracy, 'mcc': mcc, 'auc': auc},\n",
        "                  hashcode, #Hashcode for tf.events\n",
        "                  1e-2) #Learning rate"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 -- TRAINING ==============>\n",
            "EPOCH 0 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.592743, \"epoch\": 1}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.556198, \"epoch\": 1}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.970869, \"epoch\": 1}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.992692, \"epoch\": 1}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.087811, \"epoch\": 1}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.042009, \"epoch\": 1}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.521533, \"epoch\": 1}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.515237, \"epoch\": 1}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.124511, \"epoch\": 1}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.111709, \"epoch\": 1}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.746193, \"epoch\": 1}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.773776, \"epoch\": 1}\n",
            "Save done!\n",
            "1 -- TRAINING ==============>\n",
            "EPOCH 1 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.562851, \"epoch\": 2}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.546578, \"epoch\": 2}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.976465, \"epoch\": 2}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.988308, \"epoch\": 2}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.086977, \"epoch\": 2}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.056816, \"epoch\": 2}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.523857, \"epoch\": 2}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.520491, \"epoch\": 2}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.138155, \"epoch\": 2}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.123872, \"epoch\": 2}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.763382, \"epoch\": 2}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.782258, \"epoch\": 2}\n",
            "Save done!\n",
            "2 -- TRAINING ==============>\n",
            "EPOCH 2 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.554723, \"epoch\": 3}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.537785, \"epoch\": 3}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.971774, \"epoch\": 3}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.985060, \"epoch\": 3}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.104293, \"epoch\": 3}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.069049, \"epoch\": 3}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.530364, \"epoch\": 3}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.525018, \"epoch\": 3}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.152169, \"epoch\": 3}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.134698, \"epoch\": 3}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.770487, \"epoch\": 3}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.788241, \"epoch\": 3}\n",
            "Save done!\n",
            "3 -- TRAINING ==============>\n",
            "EPOCH 3 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.544841, \"epoch\": 4}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.529665, \"epoch\": 4}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.970211, \"epoch\": 4}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.986197, \"epoch\": 4}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.112514, \"epoch\": 4}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.067761, \"epoch\": 4}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.533779, \"epoch\": 4}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.524937, \"epoch\": 4}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.160112, \"epoch\": 4}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.136212, \"epoch\": 4}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.778976, \"epoch\": 4}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.793058, \"epoch\": 4}\n",
            "Save done!\n",
            "4 -- TRAINING ==============>\n",
            "EPOCH 4 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.539219, \"epoch\": 5}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.522421, \"epoch\": 5}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.968729, \"epoch\": 5}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.982624, \"epoch\": 5}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.118710, \"epoch\": 5}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.085305, \"epoch\": 5}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.536204, \"epoch\": 5}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.531970, \"epoch\": 5}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.165192, \"epoch\": 5}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.153686, \"epoch\": 5}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.782642, \"epoch\": 5}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.797302, \"epoch\": 5}\n",
            "Save done!\n",
            "5 -- TRAINING ==============>\n",
            "EPOCH 5 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.531871, \"epoch\": 6}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.514988, \"epoch\": 6}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.971198, \"epoch\": 6}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.985060, \"epoch\": 6}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.115890, \"epoch\": 6}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.082730, \"epoch\": 6}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.535982, \"epoch\": 6}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.531889, \"epoch\": 6}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.167241, \"epoch\": 6}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.157037, \"epoch\": 6}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.787210, \"epoch\": 6}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.800625, \"epoch\": 6}\n",
            "Save done!\n",
            "6 -- TRAINING ==============>\n",
            "EPOCH 6 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.522616, \"epoch\": 7}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.509585, \"epoch\": 7}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.969593, \"epoch\": 7}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.981975, \"epoch\": 7}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.127130, \"epoch\": 7}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.099952, \"epoch\": 7}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.540913, \"epoch\": 7}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.539003, \"epoch\": 7}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.178620, \"epoch\": 7}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.173612, \"epoch\": 7}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.793616, \"epoch\": 7}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.804363, \"epoch\": 7}\n",
            "Save done!\n",
            "7 -- TRAINING ==============>\n",
            "EPOCH 7 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.518567, \"epoch\": 8}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.504603, \"epoch\": 8}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.966096, \"epoch\": 8}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.984735, \"epoch\": 8}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.141388, \"epoch\": 8}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.082891, \"epoch\": 8}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.546450, \"epoch\": 8}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.531808, \"epoch\": 8}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.189096, \"epoch\": 8}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.156291, \"epoch\": 8}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.795424, \"epoch\": 8}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.807464, \"epoch\": 8}\n",
            "Save done!\n",
            "8 -- TRAINING ==============>\n",
            "EPOCH 8 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.513861, \"epoch\": 9}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.499996, \"epoch\": 9}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.966631, \"epoch\": 9}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.983761, \"epoch\": 9}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.135669, \"epoch\": 9}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.093996, \"epoch\": 9}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.543803, \"epoch\": 9}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.536901, \"epoch\": 9}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.182969, \"epoch\": 9}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.170111, \"epoch\": 9}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.798107, \"epoch\": 9}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.810562, \"epoch\": 9}\n",
            "Save done!\n",
            "9 -- TRAINING ==============>\n",
            "EPOCH 9 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.508494, \"epoch\": 10}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.498274, \"epoch\": 10}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.966137, \"epoch\": 10}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.979052, \"epoch\": 10}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.144327, \"epoch\": 10}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.114759, \"epoch\": 10}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.547966, \"epoch\": 10}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.544984, \"epoch\": 10}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.192897, \"epoch\": 10}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.186240, \"epoch\": 10}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.802609, \"epoch\": 10}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.812164, \"epoch\": 10}\n",
            "Save done!\n",
            "10 -- TRAINING ==============>\n",
            "EPOCH 10 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.504403, \"epoch\": 11}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.492255, \"epoch\": 11}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.963627, \"epoch\": 11}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.984735, \"epoch\": 11}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.154454, \"epoch\": 11}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.092387, \"epoch\": 11}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.551887, \"epoch\": 11}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.536577, \"epoch\": 11}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.199959, \"epoch\": 11}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.170613, \"epoch\": 11}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.805442, \"epoch\": 11}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.815170, \"epoch\": 11}\n",
            "Save done!\n",
            "11 -- TRAINING ==============>\n",
            "EPOCH 11 -- VALIDATION ==============>\n",
            "{\"metric\": \"train_loss\", \"value\": 0.499583, \"epoch\": 12}\n",
            "{\"metric\": \"val_loss\", \"value\": 0.488656, \"epoch\": 12}\n",
            "{\"metric\": \"train_sensitivity\", \"value\": 0.960583, \"epoch\": 12}\n",
            "{\"metric\": \"val_sensitivity\", \"value\": 0.981650, \"epoch\": 12}\n",
            "{\"metric\": \"train_specificity\", \"value\": 0.165932, \"epoch\": 12}\n",
            "{\"metric\": \"val_specificity\", \"value\": 0.105263, \"epoch\": 12}\n",
            "{\"metric\": \"train_accuracy\", \"value\": 0.556231, \"epoch\": 12}\n",
            "{\"metric\": \"val_accuracy\", \"value\": 0.541508, \"epoch\": 12}\n",
            "{\"metric\": \"train_mcc\", \"value\": 0.207374, \"epoch\": 12}\n",
            "{\"metric\": \"val_mcc\", \"value\": 0.180200, \"epoch\": 12}\n",
            "{\"metric\": \"train_auc\", \"value\": 0.810271, \"epoch\": 12}\n",
            "{\"metric\": \"val_auc\", \"value\": 0.817563, \"epoch\": 12}\n",
            "Save done!\n",
            "12 -- TRAINING ==============>\n",
            "EPOCH 12 -- VALIDATION ==============>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b350cca7ef3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                     'accuracy': accuracy, 'mcc': mcc, 'auc': auc},\n\u001b[1;32m     24\u001b[0m                   \u001b[0mhashcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#Hashcode for tf.events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                   1e-2) #Learning rate\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-3e1e243c3277>\u001b[0m in \u001b[0;36mtrain_validate\u001b[0;34m(train_dataset, val_dataset, train_device, val_device, opt_type, n_epoch, batch_size, metrics, hash_code, lr)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1c5cef3b7815>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch, x_md)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_conv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_conv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/mf_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0maggr_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aggregate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoll_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mupdate_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'update'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoll_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size,\n\u001b[0;32m--> 288\u001b[0;31m                            reduce=self.aggr)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmessage_and_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \"\"\"\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mul'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxbQ_DKnyusF"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNefTuiX3oDZ"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}