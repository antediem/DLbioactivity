{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_K_YZyThXi2S",
    "outputId": "05fe66e2-98dc-4f22-f3f3-bae7dba96133"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "add /root/miniconda/lib/python3.7/site-packages to PYTHONPATH\n",
      "rdkit is already installed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "import shutil\n",
    "from logging import getLogger, StreamHandler, INFO\n",
    "\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "logger.addHandler(StreamHandler())\n",
    "logger.setLevel(INFO)\n",
    "\n",
    "\n",
    "def install(\n",
    "        chunk_size=4096,\n",
    "        file_name=\"Miniconda3-latest-Linux-x86_64.sh\",\n",
    "        url_base=\"https://repo.continuum.io/miniconda/\",\n",
    "        conda_path=os.path.expanduser(os.path.join(\"~\", \"miniconda\")),\n",
    "        rdkit_version=None,\n",
    "        add_python_path=True,\n",
    "        force=False):\n",
    "    \"\"\"install rdkit from miniconda\n",
    "    ```\n",
    "    import rdkit_installer\n",
    "    rdkit_installer.install()\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    python_path = os.path.join(\n",
    "        conda_path,\n",
    "        \"lib\",\n",
    "        \"python{0}.{1}\".format(*sys.version_info),\n",
    "        \"site-packages\",\n",
    "    )\n",
    "\n",
    "    if add_python_path and python_path not in sys.path:\n",
    "        logger.info(\"add {} to PYTHONPATH\".format(python_path))\n",
    "        sys.path.append(python_path)\n",
    "\n",
    "    if os.path.isdir(os.path.join(python_path, \"rdkit\")):\n",
    "        logger.info(\"rdkit is already installed\")\n",
    "        if not force:\n",
    "            return\n",
    "\n",
    "        logger.info(\"force re-install\")\n",
    "\n",
    "    url = url_base + file_name\n",
    "    python_version = \"{0}.{1}.{2}\".format(*sys.version_info)\n",
    "\n",
    "    logger.info(\"python version: {}\".format(python_version))\n",
    "\n",
    "    if os.path.isdir(conda_path):\n",
    "        logger.warning(\"remove current miniconda\")\n",
    "        shutil.rmtree(conda_path)\n",
    "    elif os.path.isfile(conda_path):\n",
    "        logger.warning(\"remove {}\".format(conda_path))\n",
    "        os.remove(conda_path)\n",
    "\n",
    "    logger.info('fetching installer from {}'.format(url))\n",
    "    res = requests.get(url, stream=True)\n",
    "    res.raise_for_status()\n",
    "    with open(file_name, 'wb') as f:\n",
    "        for chunk in res.iter_content(chunk_size):\n",
    "            f.write(chunk)\n",
    "    logger.info('done')\n",
    "\n",
    "    logger.info('installing miniconda to {}'.format(conda_path))\n",
    "    subprocess.check_call([\"bash\", file_name, \"-b\", \"-p\", conda_path])\n",
    "    logger.info('done')\n",
    "\n",
    "    logger.info(\"installing rdkit\")\n",
    "    subprocess.check_call([\n",
    "        os.path.join(conda_path, \"bin\", \"conda\"),\n",
    "        \"install\",\n",
    "        \"--yes\",\n",
    "        \"-c\", \"rdkit\",\n",
    "        \"python=={}\".format(python_version),\n",
    "        \"rdkit\" if rdkit_version is None else \"rdkit=={}\".format(rdkit_version)])\n",
    "    logger.info(\"done\")\n",
    "\n",
    "    import rdkit\n",
    "    logger.info(\"rdkit-{} installation finished!\".format(rdkit.__version__))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "niebefFORugc"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from build_vocab import WordVocab\n",
    "from dataset import Seq2seqDataset\n",
    "\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "EOS = 2\n",
    "SOS = 3\n",
    "MASK = 4\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function. No batch support?\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model) # (T,H)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TrfmSeq2seq(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size, n_layers, dropout=0.1):\n",
    "        super(TrfmSeq2seq, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(in_size, hidden_size)\n",
    "        self.pe = PositionalEncoding(hidden_size, dropout)\n",
    "        self.trfm = nn.Transformer(d_model=hidden_size, nhead=4, \n",
    "        num_encoder_layers=n_layers, num_decoder_layers=n_layers, dim_feedforward=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (T,B)\n",
    "        embedded = self.embed(src)  # (T,B,H)\n",
    "        embedded = self.pe(embedded) # (T,B,H)\n",
    "        hidden = self.trfm(embedded, embedded) # (T,B,H)\n",
    "        out = self.out(hidden) # (T,B,V)\n",
    "        out = F.log_softmax(out, dim=2) # (T,B,V)\n",
    "        return out # (T,B,V)\n",
    "\n",
    "    def _encode(self, src):\n",
    "        # src: (T,B)\n",
    "        embedded = self.embed(src)  # (T,B,H)\n",
    "        embedded = self.pe(embedded) # (T,B,H)\n",
    "        output = embedded\n",
    "        for i in range(self.trfm.encoder.num_layers - 1):\n",
    "            output = self.trfm.encoder.layers[i](output, None)  # (T,B,H)\n",
    "        penul = output.detach().numpy()\n",
    "        output = self.trfm.encoder.layers[-1](output, None)  # (T,B,H)\n",
    "        if self.trfm.encoder.norm:\n",
    "            output = self.trfm.encoder.norm(output) # (T,B,H)\n",
    "        output = output.detach().numpy()\n",
    "        # mean, max, first*2\n",
    "        return np.hstack([np.mean(output, axis=0), np.max(output, axis=0), output[0,:,:], penul[0,:,:] ]) # (B,4H)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        # src: (T,B)\n",
    "        batch_size = src.shape[1]\n",
    "        if batch_size<=100:\n",
    "            return self._encode(src)\n",
    "        else: # Batch is too large to load\n",
    "            print('There are {:d} molecules. It will take a little time.'.format(batch_size))\n",
    "            st,ed = 0,100\n",
    "            out = self._encode(src[:,st:ed]) # (B,4H)\n",
    "            while ed<batch_size:\n",
    "                st += 100\n",
    "                ed += 100\n",
    "                out = np.concatenate([out, self._encode(src[:,st:ed])], axis=0)\n",
    "            return out\n",
    "\n",
    "#     return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, vocab):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for b, (sm, sm_true) in enumerate(test_loader):\n",
    "        sm = torch.t(sm.cuda()) # (T,B)\n",
    "        sm_true = torch.t(sm_true.cuda()) # (T,B)\n",
    "        with torch.no_grad():\n",
    "            output = model(sm) # (T,B,V)\n",
    "        # print(sm)\n",
    "        # print(torch.max(output, dim=2).indices)\n",
    "\n",
    "        acc = torch.eq(torch.sum(torch.where(torch.eq(sm,torch.as_tensor(PAD)), torch.as_tensor(True).cuda(), \\\n",
    "                                             torch.eq(torch.max(output, dim=2).indices, sm_true)), dim=0), sm.shape[0]).sum()\n",
    "        \n",
    "        loss = F.nll_loss(output.view(-1, len(vocab)),\n",
    "                               sm_true.contiguous().view(-1),\n",
    "                               ignore_index=PAD)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "    return total_loss / len(test_loader), total_acc / len(test_loader) / sm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PShuTyYWWvVe",
    "outputId": "10dcfdec-98fa-4839-bfd7-f553a24c1d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1kJ4Ofhjw6FH-gEvsWyvydR1R5XG3Gv6F\n",
      "To: /content/trfm_new_1_120000.pkl\n",
      "\r",
      "0.00B [00:00, ?B/s]\r",
      "2.43MB [00:00, 76.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1kJ4Ofhjw6FH-gEvsWyvydR1R5XG3Gv6F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cm3OngQtYmhu",
    "outputId": "f204ef93-e6ca-42ef-9d7e-53676a331e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1jYVXKGxya3bhiLdeaFNb20wpPXey_n1N\n",
      "To: /content/my_train_smiles.csv\n",
      "101MB [00:00, 164MB/s] \n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1jYVXKGxya3bhiLdeaFNb20wpPXey_n1N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-fbhB6XlMUk",
    "outputId": "9beec597-1c81-43b0-c515-36d729337900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tlwmS8sePg9TgMBpqeSPkMw7tOroS9FZ\n",
      "To: /content/trfm_12_23000.pkl\n",
      "22.1MB [00:00, 70.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1tlwmS8sePg9TgMBpqeSPkMw7tOroS9FZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RxTUdPbsRugq"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Args = namedtuple('Args', ['n_epoch', 'vocab', 'data', 'outdir', 'name', 'seq_len', 'batch_size', 'n_worker', 'hidden',\\\n",
    "                         'n_layer', 'n_head', 'lr', 'hpu'])\n",
    "\n",
    "args = Args(5, 'vocab.pkl', 'my_train_smiles.csv','result','ST',250,128,2,64,4,4,1e-4,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from build_vocab import WordVocab\n",
    "from dataset import Seq2seqDataset\n",
    "\n",
    "print('Loading dataset...')\n",
    "vocab = WordVocab.load_vocab(args.vocab)\n",
    "\n",
    "data = pd.read_csv(args.data)['first'].values\n",
    "\n",
    "# dataset = Seq2seqDataset(, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlist = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = []\n",
    "lengths = []\n",
    "for item in dlist:\n",
    "    lengths.append(len(item))\n",
    "    if len(item) <= 100:\n",
    "        new_data.append(item)\n",
    "new_data = np.array(new_data)\n",
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1722298,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLCEkXUIRugr",
    "outputId": "39e3ad84-3e5f-407f-d333-7aa4ff0ac2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 274861\n",
      "Train   1: iter     0 | loss 2.270 | ppl 9.683\n",
      "Train   1: iter  1000 | loss 2.206 | ppl 9.078\n",
      "Train   1: iter  2000 | loss 2.220 | ppl 9.204\n",
      "Train   1: iter  3000 | loss 2.166 | ppl 8.723\n",
      "Train   1: iter  4000 | loss 2.273 | ppl 9.704\n",
      "Train   1: iter  5000 | loss 2.191 | ppl 8.945\n",
      "Train   1: iter  6000 | loss 2.216 | ppl 9.167\n",
      "Train   1: iter  7000 | loss 2.326 | ppl 10.236\n",
      "Train   1: iter  8000 | loss 2.279 | ppl 9.766\n",
      "Train   1: iter  9000 | loss 2.206 | ppl 9.081\n",
      "Val   1: iter  9999 | loss 2.230 | ppl 9.297\n",
      "acc:  5e-05\n",
      "[!] saving model...\n",
      "Train   1: iter 10000 | loss 2.132 | ppl 8.432\n",
      "Train   1: iter 11000 | loss 2.321 | ppl 10.185\n",
      "Train   1: iter 12000 | loss 2.277 | ppl 9.750\n",
      "Train   1: iter 13000 | loss 2.260 | ppl 9.582\n",
      "Train   1: iter 14000 | loss 2.203 | ppl 9.055\n",
      "Train   1: iter 15000 | loss 2.165 | ppl 8.717\n",
      "Train   1: iter 16000 | loss 2.289 | ppl 9.863\n",
      "Train   1: iter 17000 | loss 2.181 | ppl 8.854\n",
      "Train   1: iter 18000 | loss 2.196 | ppl 8.989\n",
      "Train   1: iter 19000 | loss 2.208 | ppl 9.100\n",
      "Val   1: iter 19999 | loss 2.230 | ppl 9.299\n",
      "acc:  5e-05\n",
      "Train   1: iter 20000 | loss 2.201 | ppl 9.031\n",
      "Train   1: iter 21000 | loss 2.213 | ppl 9.147\n",
      "Train   1: iter 22000 | loss 2.140 | ppl 8.500\n",
      "Train   1: iter 23000 | loss 2.322 | ppl 10.201\n",
      "Train   1: iter 24000 | loss 2.184 | ppl 8.881\n",
      "Train   1: iter 25000 | loss 2.242 | ppl 9.408\n",
      "Train   1: iter 26000 | loss 2.166 | ppl 8.723\n",
      "Train   1: iter 27000 | loss 2.092 | ppl 8.098\n",
      "Train   1: iter 28000 | loss 2.237 | ppl 9.368\n",
      "Train   1: iter 29000 | loss 2.317 | ppl 10.149\n",
      "Val   1: iter 29999 | loss 2.230 | ppl 9.297\n",
      "acc:  5e-05\n",
      "[!] saving model...\n",
      "Train   1: iter 30000 | loss 2.226 | ppl 9.265\n",
      "Train   1: iter 31000 | loss 2.243 | ppl 9.423\n",
      "Train   1: iter 32000 | loss 2.261 | ppl 9.591\n",
      "Train   1: iter 33000 | loss 2.285 | ppl 9.828\n",
      "Train   1: iter 34000 | loss 2.194 | ppl 8.972\n",
      "Train   1: iter 35000 | loss 2.149 | ppl 8.573\n",
      "Train   1: iter 36000 | loss 2.226 | ppl 9.266\n",
      "Train   1: iter 37000 | loss 2.212 | ppl 9.136\n",
      "Train   1: iter 38000 | loss 2.250 | ppl 9.487\n",
      "Train   1: iter 39000 | loss 2.223 | ppl 9.232\n",
      "Val   1: iter 39999 | loss 2.229 | ppl 9.291\n",
      "acc:  4e-05\n",
      "[!] saving model...\n",
      "Train   1: iter 40000 | loss 2.203 | ppl 9.052\n",
      "Train   1: iter 41000 | loss 2.200 | ppl 9.021\n",
      "Train   1: iter 42000 | loss 2.260 | ppl 9.587\n",
      "Train   1: iter 43000 | loss 2.227 | ppl 9.270\n",
      "Train   1: iter 44000 | loss 2.301 | ppl 9.989\n",
      "Train   1: iter 45000 | loss 2.311 | ppl 10.085\n",
      "Train   1: iter 46000 | loss 2.163 | ppl 8.695\n",
      "Train   1: iter 47000 | loss 2.173 | ppl 8.789\n",
      "Train   1: iter 48000 | loss 2.277 | ppl 9.749\n",
      "Train   1: iter 49000 | loss 2.288 | ppl 9.851\n",
      "Val   1: iter 49999 | loss 2.229 | ppl 9.295\n",
      "acc:  3e-05\n",
      "Train   1: iter 50000 | loss 2.324 | ppl 10.221\n",
      "Train   1: iter 51000 | loss 2.198 | ppl 9.006\n",
      "Train   1: iter 52000 | loss 2.279 | ppl 9.770\n",
      "Train   1: iter 53000 | loss 2.219 | ppl 9.194\n",
      "Train   1: iter 54000 | loss 2.285 | ppl 9.823\n",
      "Train   1: iter 55000 | loss 2.226 | ppl 9.259\n",
      "Train   1: iter 56000 | loss 2.207 | ppl 9.087\n",
      "Train   1: iter 57000 | loss 2.312 | ppl 10.097\n",
      "Train   1: iter 58000 | loss 2.235 | ppl 9.348\n",
      "Train   1: iter 59000 | loss 2.164 | ppl 8.702\n",
      "Val   1: iter 59999 | loss 2.229 | ppl 9.288\n",
      "acc:  6e-05\n",
      "[!] saving model...\n",
      "Train   1: iter 60000 | loss 2.320 | ppl 10.173\n",
      "Train   1: iter 61000 | loss 2.223 | ppl 9.235\n",
      "Train   1: iter 62000 | loss 2.169 | ppl 8.746\n",
      "Train   1: iter 63000 | loss 2.203 | ppl 9.049\n",
      "Train   1: iter 64000 | loss 2.319 | ppl 10.167\n",
      "Train   1: iter 65000 | loss 2.220 | ppl 9.212\n",
      "Train   1: iter 66000 | loss 2.257 | ppl 9.557\n",
      "Train   1: iter 67000 | loss 2.139 | ppl 8.491\n",
      "Train   1: iter 68000 | loss 2.190 | ppl 8.932\n",
      "Train   1: iter 69000 | loss 2.220 | ppl 9.210\n",
      "Val   1: iter 69999 | loss 2.229 | ppl 9.289\n",
      "acc:  4e-05\n",
      "Train   1: iter 70000 | loss 2.264 | ppl 9.625\n",
      "Train   1: iter 71000 | loss 2.164 | ppl 8.703\n",
      "Train   1: iter 72000 | loss 2.206 | ppl 9.083\n",
      "Train   1: iter 73000 | loss 2.188 | ppl 8.920\n",
      "Train   1: iter 74000 | loss 2.211 | ppl 9.123\n",
      "Train   1: iter 75000 | loss 2.231 | ppl 9.307\n",
      "Train   1: iter 76000 | loss 2.262 | ppl 9.606\n",
      "Train   1: iter 77000 | loss 2.262 | ppl 9.606\n",
      "Train   1: iter 78000 | loss 2.333 | ppl 10.308\n",
      "Train   1: iter 79000 | loss 2.212 | ppl 9.130\n",
      "Val   1: iter 79999 | loss 2.229 | ppl 9.286\n",
      "acc:  3e-05\n",
      "[!] saving model...\n",
      "Train   1: iter 80000 | loss 2.219 | ppl 9.194\n",
      "Train   1: iter 81000 | loss 2.251 | ppl 9.495\n",
      "Train   1: iter 82000 | loss 2.284 | ppl 9.815\n",
      "Train   1: iter 83000 | loss 2.270 | ppl 9.678\n",
      "Train   1: iter 84000 | loss 2.267 | ppl 9.653\n",
      "Train   1: iter 85000 | loss 2.171 | ppl 8.768\n",
      "Train   1: iter 86000 | loss 2.295 | ppl 9.927\n",
      "Train   1: iter 87000 | loss 2.266 | ppl 9.645\n",
      "Train   1: iter 88000 | loss 2.253 | ppl 9.519\n",
      "Train   1: iter 89000 | loss 2.141 | ppl 8.506\n",
      "Val   1: iter 89999 | loss 2.228 | ppl 9.285\n",
      "acc:  2e-05\n",
      "[!] saving model...\n",
      "Train   1: iter 90000 | loss 2.232 | ppl 9.316\n",
      "Train   1: iter 91000 | loss 2.280 | ppl 9.779\n",
      "Train   1: iter 92000 | loss 2.151 | ppl 8.589\n",
      "Train   1: iter 93000 | loss 2.282 | ppl 9.795\n",
      "Train   1: iter 94000 | loss 2.208 | ppl 9.096\n",
      "Train   1: iter 95000 | loss 2.265 | ppl 9.629\n",
      "Train   1: iter 96000 | loss 2.314 | ppl 10.119\n",
      "Train   1: iter 97000 | loss 2.173 | ppl 8.784\n",
      "Train   1: iter 98000 | loss 2.252 | ppl 9.510\n",
      "Train   1: iter 99000 | loss 2.280 | ppl 9.776\n",
      "Val   1: iter 99999 | loss 2.228 | ppl 9.280\n",
      "acc:  5e-05\n",
      "[!] saving model...\n",
      "Train   1: iter 100000 | loss 2.220 | ppl 9.209\n",
      "Train   1: iter 101000 | loss 2.116 | ppl 8.295\n",
      "Train   2: iter     0 | loss 2.247 | ppl 9.457\n",
      "Train   2: iter  1000 | loss 2.260 | ppl 9.579\n",
      "Train   2: iter  2000 | loss 2.278 | ppl 9.762\n",
      "Train   2: iter  3000 | loss 2.208 | ppl 9.097\n",
      "Train   2: iter  4000 | loss 2.273 | ppl 9.707\n",
      "Train   2: iter  5000 | loss 2.194 | ppl 8.969\n",
      "Train   2: iter  6000 | loss 2.203 | ppl 9.049\n",
      "Train   2: iter  7000 | loss 2.262 | ppl 9.606\n",
      "Train   2: iter  8000 | loss 2.260 | ppl 9.587\n",
      "Train   2: iter  9000 | loss 2.216 | ppl 9.167\n",
      "Val   2: iter  9999 | loss 2.228 | ppl 9.283\n",
      "acc:  5e-05\n",
      "Train   2: iter 10000 | loss 2.289 | ppl 9.869\n",
      "Train   2: iter 11000 | loss 2.315 | ppl 10.125\n",
      "Train   2: iter 12000 | loss 2.339 | ppl 10.366\n",
      "Train   2: iter 13000 | loss 2.260 | ppl 9.580\n",
      "Train   2: iter 14000 | loss 2.202 | ppl 9.045\n",
      "Train   2: iter 15000 | loss 2.162 | ppl 8.692\n",
      "Train   2: iter 16000 | loss 2.150 | ppl 8.581\n",
      "Train   2: iter 17000 | loss 2.327 | ppl 10.243\n",
      "Train   2: iter 18000 | loss 2.236 | ppl 9.352\n",
      "Train   2: iter 19000 | loss 2.239 | ppl 9.385\n",
      "Val   2: iter 19999 | loss 2.228 | ppl 9.280\n",
      "acc:  6e-05\n",
      "[!] saving model...\n",
      "Train   2: iter 20000 | loss 2.190 | ppl 8.937\n",
      "Train   2: iter 21000 | loss 2.273 | ppl 9.706\n",
      "Train   2: iter 22000 | loss 2.223 | ppl 9.231\n",
      "Train   2: iter 23000 | loss 2.296 | ppl 9.933\n",
      "Train   2: iter 24000 | loss 2.369 | ppl 10.692\n",
      "Train   2: iter 25000 | loss 2.337 | ppl 10.348\n",
      "Train   2: iter 26000 | loss 2.274 | ppl 9.722\n",
      "Train   2: iter 27000 | loss 2.256 | ppl 9.542\n",
      "Train   2: iter 28000 | loss 2.288 | ppl 9.855\n",
      "Train   2: iter 29000 | loss 2.123 | ppl 8.360\n",
      "Val   2: iter 29999 | loss 2.228 | ppl 9.278\n",
      "acc:  7e-05\n",
      "[!] saving model...\n",
      "Train   2: iter 30000 | loss 2.277 | ppl 9.744\n",
      "Train   2: iter 31000 | loss 2.259 | ppl 9.571\n",
      "Train   2: iter 32000 | loss 2.284 | ppl 9.811\n",
      "Train   2: iter 33000 | loss 2.220 | ppl 9.203\n",
      "Train   2: iter 34000 | loss 2.255 | ppl 9.531\n",
      "Train   2: iter 35000 | loss 2.313 | ppl 10.106\n",
      "Train   2: iter 36000 | loss 2.171 | ppl 8.767\n",
      "Train   2: iter 37000 | loss 2.164 | ppl 8.705\n",
      "Train   2: iter 38000 | loss 2.203 | ppl 9.054\n",
      "Train   2: iter 39000 | loss 2.218 | ppl 9.190\n",
      "Val   2: iter 39999 | loss 2.228 | ppl 9.283\n",
      "acc:  6e-05\n",
      "Train   2: iter 40000 | loss 2.186 | ppl 8.897\n",
      "Train   2: iter 41000 | loss 2.247 | ppl 9.456\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "\n",
    "print('Loading dataset...')\n",
    "vocab = WordVocab.load_vocab(args.vocab)\n",
    "dataset = Seq2seqDataset(pd.read_csv(args.data)['first'].values, vocab)\n",
    "test_size = 100000\n",
    "train, test = torch.utils.data.random_split(dataset, [len(dataset)-test_size, test_size])\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, num_workers=args.n_worker)\n",
    "test_loader = DataLoader(test, batch_size=args.batch_size, shuffle=False, num_workers=args.n_worker)\n",
    "print('Train size:', len(train))\n",
    "print('Test size:', len(test))\n",
    "del dataset, train, test\n",
    "\n",
    "model = TrfmSeq2seq(len(vocab), args.hidden, len(vocab), args.n_layer).cuda()\n",
    "\n",
    "# model.load_state_dict(torch.load(os.path.normpath('trfm_new_1_120000.pkl')))\n",
    "\n",
    "# model.load_state_dict(torch.load(os.path.normpath('trfm_12_23000.pkl')))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "print(model)\n",
    "print('Total parameters:', sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# best_loss = None\n",
    "for e in range(1, args.n_epoch):\n",
    "    for b, (sm, sm_true) in enumerate(train_loader):\n",
    "        sm = torch.t(sm.cuda()) # (T,B)\n",
    "        sm_true = torch.t(sm_true.cuda()) # (T,B)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sm) # (T,B,V)\n",
    "        # print(sm_true, sm)\n",
    "\n",
    "        loss = F.nll_loss(output.view(-1, len(vocab)),\n",
    "                sm_true.contiguous().view(-1), ignore_index=PAD)\n",
    "        # assert False\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if b%1000==0:\n",
    "            print('Train {:3d}: iter {:5d} | loss {:.3f} | ppl {:.3f}'.format(e, b, loss.item(), math.exp(loss.item())))\n",
    "        if (b+1)%10000==0:\n",
    "            loss,acc = evaluate(model, test_loader, vocab)\n",
    "            print('Val {:3d}: iter {:5d} | loss {:.3f} | ppl {:.3f}'.format(e, b, loss, math.exp(loss)))\n",
    "            print('acc: ', acc)\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if not best_loss or loss < best_loss:\n",
    "                print(\"[!] saving model...\")\n",
    "                if not os.path.isdir(\"save\"):\n",
    "                    os.makedirs(\"save\")\n",
    "                torch.save(model.state_dict(), './drive/MyDrive/FinalProject/trfm_new_%d_%d.pkl' % (e,b))\n",
    "                best_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3werCHjgHkX",
    "outputId": "59aeb5a5-6bca-4d48-d549-99fb3eb500f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EfLWtBfbgSMn"
   },
   "outputs": [],
   "source": [
    "!cp save/trfm_new_1_99999.pkl  drive/MyDrive/FinalProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ie86EpZRugx",
    "outputId": "f1e9c704-5c61-48e3-e32e-e1b1ea44bbaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 1., 1., 1.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 0., 0., 1., 1.]]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "output = torch.zeros(2,3,5) + torch.ge(torch.randn(2,3,5), torch.as_tensor(0.5))\n",
    "print(output)\n",
    "sm = torch.ones(2,3)\n",
    "print(sm)\n",
    "torch.eq(torch.sum(torch.eq(torch.max(output, dim=2).values, sm), dim=0), sm.shape[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTzuv-68Rugz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TransformerTrainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
